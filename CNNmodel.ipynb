{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "595b6845-1ac0-42e3-be9e-29c15442ccdc",
    "_uuid": "300eb65ae193e447250f8d7d0ca2b0867a9ff707",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# #Pretrained Model - Word Embedding - Word2Vec\n",
    "# from nltk.data import find\n",
    "# word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "# word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('W2V_Model/GoogleNews-vectors-negative300.bin/data', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "1a7aa1ee-594e-42d7-9f30-0538b7e88919",
    "_uuid": "aac70ce16fd4d9cc2393727e29f098300fc77d8d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 6838  samples\n",
      "Test Size: 886  samples\n"
     ]
    }
   ],
   "source": [
    "# Importing Train and Test Data\n",
    "import csv\n",
    "\n",
    "emotions = ['anger', \n",
    "            # 'anticipation', \n",
    "            'disgust', \n",
    "            'fear', \n",
    "            'joy', \n",
    "            # 'love',\n",
    "            'optimism', \n",
    "            'pessimism', \n",
    "            'sadness', \n",
    "            'surprise'] #, \n",
    "            # 'trust']\n",
    "\n",
    "x_train_raw = []\n",
    "y_train_raw = []\n",
    "x_test_raw = []\n",
    "y_test_raw = []\n",
    "\n",
    "\n",
    "with open('Data/train.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        x_train_raw.append(row['Tweet'])\n",
    "        y_train_raw_temp = []\n",
    "        for emotion in emotions:\n",
    "            y_train_raw_temp.append(row[emotion])\n",
    "        y_train_raw.append(y_train_raw_temp)\n",
    "        \n",
    "with open('Data/test.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        x_test_raw.append(row['Tweet'])\n",
    "        y_test_raw_temp = []\n",
    "        for emotion in emotions:\n",
    "            y_test_raw_temp.append(row[emotion])\n",
    "        y_test_raw.append(y_test_raw_temp)\n",
    "        \n",
    "train_size = len(y_train_raw)\n",
    "test_size = len(y_test_raw)\n",
    "\n",
    "print(\"Train Size:\", train_size, \" samples\")\n",
    "print(\"Test Size:\", test_size, \" samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "55e3ea36-17fa-436e-a5d9-6f5d3de1401b",
    "_uuid": "b5a045613fbd611fd0153ad9d76b1633087d35bc"
   },
   "outputs": [],
   "source": [
    "SentiWords = {}\n",
    "with open('Data/words.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        w = row['Words']\n",
    "        SentiWords_temp = np.zeros(len(emotions), dtype=K.floatx())\n",
    "        for i, emotion in enumerate(emotions):\n",
    "            SentiWords_temp[i] = row[emotion]\n",
    "        SentiWords[w] = SentiWords_temp\n",
    "    SentiWords[\"QQ\"] = np.zeros(len(emotions), dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1378401-da11-4a63-bc5f-173ce56c4cb1",
    "_uuid": "6626402b6485e0d5e609fed2f47df3d7a5578a1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3000000  words\n",
      "Word-Vector size used:  300\n"
     ]
    }
   ],
   "source": [
    "# initialization Block\n",
    "max_n_words = 40\n",
    "ignore_words = ['?', '@', '-', '.', '_', '/', ' ', '.', '!',\n",
    "               \"you'll\", 'itself', 'some', 'same', 'off', 'any', 'having',\n",
    "                'and', 'theirs', 'your', 'should', 'after', 'out', 'in', \n",
    "                \"you'd\", 'd', 'its', 'had', 'myself','from', 'ourselves', \n",
    "                'here', 'an', 'all', 'yours', 'as', 'hers', 'they', 'll',\n",
    "                \"she's\", 'through', 'you', 'then', 'once', 'my', 'am', 'who',\n",
    "                'being', 'of', 'shan', 'that', 'so', 'with', 'yourselves',\n",
    "                'both', 't', 'his', 'we', 'more', 'did', 'our', 'he', 'o', \n",
    "                'them', 'than', 'it', 'y', 'her', 'up', 'about', 'this', \n",
    "                'himself', 'just', 'if', 'own', 'has', 'how', 'because', \n",
    "                'him', 'doing', 'at', 'm', 'is', 'each', 's', 'too', 'those', \n",
    "                'such', 'have', 'above', \"you've\", 'most', 'on', 'under', \n",
    "                'by', 'few', 'where', 'when', 'were', \"you're\", \"it's\",\n",
    "                'been', 'the', 'before', 'do', 'these', 'other', 'to', 'i',\n",
    "                'can', 'themselves', 'what', 'are', 'while', 'which', 'me',\n",
    "                'ma', \"that'll\", 've', 'for', 'why', 'a', 'during', 'yourself',\n",
    "                'below', 'now', 'only', 'their', 'herself', 'will', 'does', \n",
    "                'she', 'be', 'there', \"should've\", 'was', 're', 'ours', \n",
    "                'whom', 'further']\n",
    "\n",
    "# ignore_words = ['?', '@', '-', '.', '_', '/', ' ', '.', '!']\n",
    "\n",
    "vector_size = len(word_embedding_model['I'])\n",
    "\n",
    "print (\"Vocabulary size: \",len(word_embedding_model.vocab), \" words\")\n",
    "\n",
    "print (\"Word-Vector size used: \", vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0cfac649-e15d-499b-ab3e-7456c884fc8e",
    "_uuid": "cb20f38c98fa0df96bfddfd3d70a9f7c324c949a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorized\n",
      "Input shape:  (6838, 40, 308)\n",
      "Output shape:  (6838, 8)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and vectorizing the data\n",
    "max_n_words=40\n",
    "\n",
    "vector_size = len(word_embedding_model['I'])\n",
    "# Lemmatizing Functions\n",
    "def get_wordnet_pos(tag):\n",
    "    if (tag == ''):\n",
    "        return ''\n",
    "    elif tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def lemmatize_w(word, tag):\n",
    "    wn_tag = get_wordnet_pos(tag)\n",
    "    if (wn_tag == ''):\n",
    "        return word\n",
    "    else:\n",
    "        return WordNetLemmatizer().lemmatize(word, wn_tag)\n",
    "\n",
    "def lemmatize_s(sentence):\n",
    "#     sent = []\n",
    "#     word_tag = nltk.pos_tag(sentence)\n",
    "    \n",
    "#     for w, t in word_tag:\n",
    "#         sent.append(lemmatize_w(w, t))\n",
    "#     return sent\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# Filtering the data\n",
    "def filter_sent (sentence, embedding_model = word_embedding_model):\n",
    "    \n",
    "    # tokenize each word in the sentence\n",
    "    s_words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    \n",
    "    l = lemmatize_s(s_words)\n",
    "    \n",
    "    for w in l:\n",
    "        # Remove words not in Vocab\n",
    "        if w in embedding_model.vocab:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    return filtered_sentence\n",
    "\n",
    "# Converting data to vectors\n",
    "x_train = np.zeros((train_size, max_n_words, vector_size + len(emotions)), dtype=K.floatx())\n",
    "y_train = np.zeros((train_size, len(emotions)), dtype=np.int32)\n",
    "x_test = np.zeros((test_size, max_n_words, vector_size + len(emotions)), dtype=K.floatx())\n",
    "y_test = np.zeros((test_size, len(emotions)), dtype=np.int32)\n",
    "\n",
    "for i in range(train_size):\n",
    "    x = filter_sent(x_train_raw[i])\n",
    "    for index, word in enumerate(x):\n",
    "        if word in SentiWords:\n",
    "            vec = SentiWords[word]\n",
    "        else:\n",
    "            vec = SentiWords[\"QQ\"]\n",
    "        x_train[i, index, :] = np.concatenate((word_embedding_model[word], vec), axis=0)\n",
    "        \n",
    "for i, y in enumerate(y_train_raw):\n",
    "    y_temp = np.zeros(len(emotions), dtype=np.int32)\n",
    "    y_temp = np.array(y, dtype=np.int32)\n",
    "    y_train[i, :] = y_temp\n",
    "        \n",
    "for i in range(test_size):\n",
    "    x = filter_sent(x_test_raw[i])\n",
    "    for index, word in enumerate(x):\n",
    "        if word in SentiWords:\n",
    "            vec = SentiWords[word]\n",
    "        else:\n",
    "            vec = SentiWords[\"QQ\"]\n",
    "        x_test[i, index, :] = np.concatenate((word_embedding_model[word], vec), axis=0)\n",
    "\n",
    "        \n",
    "for i, y in enumerate(y_test_raw):\n",
    "    y_temp = np.zeros(len(emotions), dtype=np.int32)\n",
    "    y_temp = np.array(y, dtype=np.int32)\n",
    "    y_test[i, :] = y_temp\n",
    "    \n",
    "print (\"Data Vectorized\")\n",
    "\n",
    "print (\"Input shape: \", x_train.shape)\n",
    "print (\"Output shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "ab46f426-ab80-4ef1-bf84-f88985d1cada",
    "_uuid": "9167aac618c1fdd532f0be934babae15fb48bf31",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "MODEL:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 64)            59200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 40, 64)            12352     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 32)            4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 20, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                20544     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 100,648\n",
      "Trainable params: 100,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Embedding\n",
    "\n",
    "# Keras model\n",
    "batch_size = 32  # 10\n",
    "nb_epochs = 10   # 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# CNN\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same', \n",
    "                 input_shape=(max_n_words, vector_size + len(emotions))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))  # tanh # relu\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(len(emotions), activation='sigmoid'))  # softmax # \n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-7),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(\"MODEL:\")\n",
    "print(model.summary(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "3fc6c0f1-8cec-4e29-b87d-2e60dd2e6fe4",
    "_uuid": "326343034e5d41dfe020414d07705a708ba7afd6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ZEESHAN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6838 samples, validate on 886 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.5330 - acc: 0.7414 - val_loss: 0.4605 - val_acc: 0.8081\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.4648 - acc: 0.7907 - val_loss: 0.4273 - val_acc: 0.8200\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.4424 - acc: 0.8042 - val_loss: 0.4138 - val_acc: 0.8239\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4103 - val_acc: 0.8238\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.4188 - acc: 0.8183 - val_loss: 0.4080 - val_acc: 0.8232\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.4128 - acc: 0.8192 - val_loss: 0.4050 - val_acc: 0.8244\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.4013 - acc: 0.8253 - val_loss: 0.4038 - val_acc: 0.8207\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.3911 - acc: 0.8308 - val_loss: 0.3982 - val_acc: 0.8235\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.3845 - acc: 0.8333 - val_loss: 0.3957 - val_acc: 0.8284\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.3785 - acc: 0.8376 - val_loss: 0.3927 - val_acc: 0.8249\n",
      "\n",
      "================================== Model Trained =================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "#x_train = np.zeros((train_size, max_n_words, vector_size + len(emotions)), dtype=K.floatx())\n",
    "#y_train = np.zeros((train_size, len(emotions)), dtype=np.int32)\n",
    "#x_test = np.zeros((test_size, max_n_words, vector_size + len(emotions)), dtype=K.floatx())\n",
    "#y_test = np.zeros((test_size, len(emotions)), dtype=np.int32)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False, #False,\n",
    "                    epochs=nb_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[EarlyStopping(min_delta=1e-7, patience=3)])  # min_delta=0.00025, patience=2\n",
    "\n",
    "# Fit the model (without early stop)\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     shuffle=False, #False\n",
    "#                     epochs=nb_epochs,\n",
    "#                     verbose=2,\n",
    "#                     validation_data=(x_test, y_test))\n",
    "\n",
    "print (\"\\n================================== Model Trained =================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "7744a9f0-b6c2-4d26-826b-f7a5cd6377b2",
    "_uuid": "5c7366dd052ced9ad982fcd02655647c55083ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "f41b9902-6aa4-4828-871c-48799e47a761",
    "_uuid": "6aebee321ad13e91de4fbee609f2db5b74c154a0"
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "ERROR_THRESHOLD = 0.40\n",
    "\n",
    "def classify(sentence, model, show_details=False):\n",
    "    \n",
    "    inp = np.zeros((train_size, max_n_words, vector_size + len(emotions)), dtype=K.floatx())\n",
    "    \n",
    "    filtered_sentence = filter_sent(sentence)\n",
    "    x = filtered_sentence\n",
    "        \n",
    "    for index, word in enumerate(x):\n",
    "        if word in SentiWords:\n",
    "            vec = SentiWords[word]\n",
    "        else:\n",
    "            vec = SentiWords[\"QQ\"]\n",
    "        inp[0, index, :] = np.concatenate((word_embedding_model[word], vec), axis=0)\n",
    "    \n",
    "    results = model.predict(x = inp, batch_size=None)\n",
    "    \n",
    "    results2 = [[i,r] for i,r in enumerate(results[0]) if r > ERROR_THRESHOLD ] \n",
    "    results2.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_results = [[emotions[r[0]],r[1]] for r in results2]\n",
    "    \n",
    "    if show_details:\n",
    "        print (emotions)\n",
    "        print (results[0])\n",
    "    \n",
    "    if len(return_results) == 0:\n",
    "        return_results.append(\"Neutral\")\n",
    "        \n",
    "    return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "cfc7bc28-0b35-4474-84f1-eb5d390572bd",
    "_uuid": "423cd271212af027632efc60bbfeb458366b678d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is so bad \n",
      "\n",
      "Prediction:  [['sadness', 0.50519097], ['disgust', 0.4918079], ['anger', 0.40325317]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Testing\n",
    "test_x = \"He is so bad\"\n",
    "print (test_x, \"\\n\")\n",
    "print (\"Prediction: \", classify(test_x, model), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "0da0ac88-a1b6-489a-813a-b9aa286deb6f",
    "_uuid": "9485fac2da0ed8b1a44ebefc0f9687e81c9ee701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even in an incomplete state , the ten-tails possesses immense physical strength and is remarkably fast , despite its tremendous size \n",
      "\n",
      "Actual Tag:  ['joy', 'optimism', 'surprise'] \n",
      "\n",
      "Prediction:  [['joy', 0.91564876], ['optimism', 0.7865144]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random manual Testing\n",
    "n = np.random.randint(test_size - 1)\n",
    "test_x = x_test_raw[n]\n",
    "\n",
    "# n = np.random.randint(train_size - 1)\n",
    "# test_x = x_train_raw[n]\n",
    "\n",
    "print (test_x,\"\\n\")\n",
    "\n",
    "tags = []\n",
    "for i, val in enumerate(y_test_raw[n]):\n",
    "    if val == '1':\n",
    "        tags.append(emotions[i])\n",
    "print (\"Actual Tag: \", tags, \"\\n\")\n",
    "\n",
    "# for i, val in enumerate(y_train_raw[n]):\n",
    "#     if val == '1':\n",
    "#         tags.append(emotions[i])\n",
    "# print (\"Actual Tag: \", tags)\n",
    "\n",
    "print (\"Prediction: \", classify(test_x, model), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "42365206-fd96-4a3f-9505-a48b1d10a46d",
    "_uuid": "263c057433f25a01c5d3037109f57f6a2b90f119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Saving Model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/model_CNN_miltilabel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Models/model_CNN_miltilabel.h5\")\n",
    "print (\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "3ab20cbc-994b-4241-a323-c79521ba9cbb",
    "_uuid": "51485e82b2563ac075b2fd4d7d74ac9c42ffb4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Loading Model\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('Models/model_CNN_miltilabel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Models/model_CNN_miltilabel.h5\")\n",
    "print (\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "7be73888-b59e-4269-8d55-22565df872c6",
    "_uuid": "1eafbf127139ea05050e3080f9fa87be273a7b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886/886 [==============================] - 0s 498us/step\n",
      "acc: 82.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate loaded model on test data\n",
    "loaded_model.compile(loss = 'binary_crossentropy', \n",
    "                     optimizer = RMSprop(lr = 0.002, rho = 0.9, epsilon = None, decay = 1e-7),\n",
    "                     metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
    "print (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "4acbb9ca-c26c-419d-b0fc-a2f55467d328",
    "_uuid": "457d1daf8838e7d633d4b0aca25777907bd16629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text in English: Hello brother.. how are u???\n",
      "Hello brother.. how are u??? \n",
      "\n",
      "Prediction:  ['Neutral'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Testing\n",
    "test_x = input(\"Enter the text in English: \")\n",
    "print (test_x, \"\\n\")\n",
    "print (\"Prediction: \", classify(test_x, loaded_model),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
