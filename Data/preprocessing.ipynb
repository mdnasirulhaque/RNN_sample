{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Library Call\n",
    "import re\n",
    "import nltk\n",
    "import html\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Cleaning functions\n",
    "\n",
    "def init(text):\n",
    "# from html.parser import HTMLParser\n",
    "\n",
    "# html_parser = HTMLParser()\n",
    "    original_tweet = text\n",
    "    text = html.unescape(original_tweet)\n",
    "    text = text.replace(\"\\\\n\", \". \")    # Removed new line character from input data\n",
    "#     print (\"\\n0. \", text)\n",
    "    return text\n",
    "\n",
    "## Decoding Data\n",
    "# tweet = original_tweet.decode(\"utf8\").encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "def removeUnnecessaryWords(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    appostophes = {\"'s\" : \"is\", \"'re\" : \"are\", \"'ve\" : \"have\", \"'d\" : \"would\",\n",
    "                   \"'m\" : \"am\", \"'t\" : \"not\", \"'ll\" : \"shall\", \"ca\" : \"can\", \"n't\" : \"not\"} \n",
    "        \n",
    "    # More to the list must be added\n",
    "\n",
    "    ignore_words = ['?', '-', '.', '_', '/', ' ', '.', '!', '\\n', '#'] #,\n",
    "#                \"you'll\", 'itself', 'some', 'same', 'off', 'any', 'having',\n",
    "#                 'and', 'theirs', 'your', 'should', 'after', 'out', 'in', \n",
    "#                 \"you'd\", 'd', 'its', 'had', 'myself','from', 'ourselves', \n",
    "#                 'here', 'an', 'all', 'yours', 'as', 'hers', 'they', 'll',\n",
    "#                 \"she's\", 'through', 'you', 'then', 'once', 'my', 'am', 'who',\n",
    "#                 'being', 'of', 'shan', 'that', 'so', 'with', 'yourselves',\n",
    "#                 'both', 't', 'his', 'we', 'more', 'did', 'our', 'he', 'o', \n",
    "#                 'them', 'than', 'it', 'y', 'her', 'up', 'about', 'this', \n",
    "#                 'himself', 'just', 'if', 'own', 'has', 'how', 'because', \n",
    "#                 'him', 'doing', 'at', 'm', 'is', 'each', 's', 'too', 'those', \n",
    "#                 'such', 'have', 'above', \"you've\", 'most', 'on', 'under', \n",
    "#                 'by', 'few', 'where', 'when', 'were', \"you're\", \"it's\",\n",
    "#                 'been', 'the', 'before', 'do', 'these', 'other', 'to', 'i',\n",
    "#                 'can', 'themselves', 'what', 'are', 'while', 'which', 'me',\n",
    "#                 'ma', \"that'll\", 've', 'for', 'why', 'a', 'during', 'yourself',\n",
    "#                 'below', 'now', 'only', 'their', 'herself', 'will', 'does', \n",
    "#                 'she', 'be', 'there', \"should've\", 'was', 're', 'ours', \n",
    "#                 'whom', 'further',' ']\n",
    "\n",
    "    # Appostophes word removal\n",
    "    reformed = [appostophes[word] if word in appostophes else word for word in words]\n",
    "#     print (\"1.1. \", reformed, \"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Hashtags and @References Removed\n",
    "    i = 0\n",
    "    while(i < len(reformed)):\n",
    "        word = reformed[i]\n",
    "#         if word == \"#\" or word == \"@\":\n",
    "        if word == \"@\":\n",
    "            if i+1 < len(reformed):\n",
    "                del reformed[i+1]\n",
    "            del reformed[i]\n",
    "            i -= 1\n",
    "        i += 1\n",
    "            \n",
    "#     print(\"1.2. \", reformed, \"\\n\")\n",
    "    \n",
    "    # Turn all caps words (E.g: 'WHAT') to lowercases\n",
    "    reformed2 = []\n",
    "    for i, word in enumerate(reformed):\n",
    "        if re.match('[A-Z]+', word):\n",
    "            reformed2 = reformed[:i]\n",
    "            reformed2.append(reformed[i].lower())\n",
    "            reformed = reformed2 + reformed[i+1:]\n",
    "#     print(\"1.3. \", reformed, \"\\n\")\n",
    "    \n",
    "    # Stop words and punctuation removal\n",
    "    reformed2 = []\n",
    "    for i, word in enumerate(reformed):\n",
    "        if word.lower() not in ignore_words:\n",
    "            reformed2.append(word)\n",
    "    reformed = \" \".join(reformed2)\n",
    "#     print (\"1.4. \", reformed, \"\\n\") \n",
    "    return reformed\n",
    "\n",
    "\n",
    "\n",
    "def splitAttachedWords(text):\n",
    "    # Fixing the lower cased front removed error, by changing the 1st 'letter' to uppercase\n",
    "    for i, c in enumerate(text):\n",
    "        if (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z'):\n",
    "            text = text[:i] + text[i].upper() + text[i+1:]\n",
    "            break\n",
    "    \n",
    "    cleaned = \" \".join(re.findall('[A-Z][^A-Z]*', text))\n",
    "#     print (\"2. \", cleaned, \"\\n\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "def standardizeWords(text):\n",
    "    cleaned = text\n",
    "    import itertools\n",
    "    cleaned = ''.join(''.join(s)[:2] for _, s in itertools.groupby(cleaned))\n",
    "#     print (\"3. \", cleaned, \"\\n\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "def removeURLs(text):\n",
    "    cleaned = text\n",
    "    url_reg  = r'[a-z]*[:.]+\\S+'\n",
    "    result   = re.sub(url_reg, '', cleaned)\n",
    "#     print (\"4. \", result, \"\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single function defining the whole cleaning process\n",
    "def clean_tweet(text):\n",
    "    text = init(text)\n",
    "    text = removeUnnecessaryWords(text)\n",
    "    text = splitAttachedWords(text)\n",
    "    text = standardizeWords(text)\n",
    "    text = removeURLs(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_file(inpFile, outFile):\n",
    "    # Initializing\n",
    "    tweets = []\n",
    "    anger = []\n",
    "    anticipation = []\n",
    "    disgust = []\n",
    "    fear = []\n",
    "    joy = []\n",
    "    love = []\n",
    "    optimism = []\n",
    "    pessimism = []\n",
    "    sadness = []\n",
    "    surprise = []\n",
    "    trust = []\n",
    "\n",
    "\n",
    "    # Reading file\n",
    "    with open(inpFile) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            tweets.append(row['Tweet'])\n",
    "            anger.append(row['anger'])\n",
    "            anticipation.append(row['anticipation'])\n",
    "            disgust.append(row['disgust'])\n",
    "            fear.append(row['fear'])\n",
    "            joy.append(row['joy'])\n",
    "            love.append(row['love'])\n",
    "            optimism.append(row['optimism'])\n",
    "            pessimism.append(row['pessimism'])\n",
    "            sadness.append(row['sadness'])\n",
    "            surprise.append(row['surprise'])\n",
    "            trust.append(row['trust'])\n",
    "\n",
    "    # Cleaning the tweets\n",
    "    cleaned_tweets = []\n",
    "    for line in tweets:\n",
    "        cleaned_tweets.append(clean_tweet(line))\n",
    "\n",
    "\n",
    "    # Building ouput\n",
    "    out = []\n",
    "    for i, _ in enumerate(cleaned_tweets):\n",
    "        out.append([cleaned_tweets[i], anger[i], anticipation[i], disgust[i], fear[i], joy[i], love[i], \n",
    "                        optimism[i], pessimism[i], sadness[i], surprise[i], trust[i]])\n",
    "\n",
    "\n",
    "    # out = np.array([tweets, anger, anticipation, disgust, fear, joy, love, \n",
    "    #                     optimism, pessimism, sadness, surprise, trust])\n",
    "    # out.transpose() \n",
    "    #\n",
    "    # I am sure there is a way to avoide the array above using transpose of a matrix, but... I will pass\n",
    "\n",
    "    outputFile = open(outFile, 'w')\n",
    "    with outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        writer.writerows([['Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', \n",
    "                           'optimism', 'pessimism', 'sadness', 'surprise', 'trust']])\n",
    "        writer.writerows(out)\n",
    "\n",
    "    print(\"Writing complete\")\n",
    "\n",
    "    outputFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing complete\n",
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "clean_file('SemEval_Train.csv', 'train.csv')\n",
    "clean_file('SemEval_Dev.csv', 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
